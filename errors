2025-10-03T04:18:56.330470Z [debug    ] HTTP Request                   [chatter.main] body={"message":"what time is it","conversation_id":"01K6JWZP1QGCAPDBF68FB5T9YN","workflow_config":{"enable_retrieval":false,"enable_tools":true,"enable_memory":false,"llm_config":{"temperature":2,"max_tokens":2048},"tool_config":{"enabled":true,"max_tool_calls":1,"parallel_tool_calls":true,"allowed_tools":["get_time","calculator"]},"retrieval_config":{"enabled":true,"rerank":true}},"profile_id":"01K6B5H7V7W1YMMJK90BRJBMT7","enable_tracing":false,"enable_retrieval":false,"enable_tools":true,"enable_memory":false,"enable_web_search":false} headers={'host': 'localhost:8000', 'connection': 'keep-alive', 'content-length': '539', 'sec-ch-ua-platform': '"Windows"', 'authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIwMUs2QjVIN1NYNFoyNjBKQ1JDMDhSWTM5QSIsImp0aSI6IjAxSzZNNFBLU0JYRDNXSDhUVDVOOVJUQURNIiwidHlwZSI6ImFjY2VzcyIsImlhdCI6MTc1OTQ2NTEzMSwic2Vzc2lvbl9pZCI6IjAxSzZFSzYxNUpGMFY0Qk5XUVJEWUdTNTZDIiwiZXhwIjoxNzU5NDY2OTMxfQ.jr82CLur-AC_KzlozGdGuX0j2RZuKn8ULSUEJ0jeBFU', 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36', 'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"', 'content-type': 'application/json', 'sec-ch-ua-mobile': '?0', 'accept': '*/*', 'origin': 'http://localhost:3000', 'sec-fetch-site': 'same-site', 'sec-fetch-mode': 'cors', 'sec-fetch-dest': 'empty', 'referer': 'http://localhost:3000/', 'accept-encoding': 'gzip, deflate, br, zstd', 'accept-language': 'en-US,en;q=0.9', 'cookie': 'refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIwMUs2QjVIN1NYNFoyNjBKQ1JDMDhSWTM5QSIsImp0aSI6IjAxSzZNNFBLU0JYRDNXSDhUVDVOOVJUQURNIiwidHlwZSI6InJlZnJlc2giLCJpYXQiOjE3NTk0NjUxMzEsInNlc3Npb25faWQiOiIwMUs2RUs2MTVKRjBWNEJOV1FSRFlHUzU2QyIsImV4cCI6MTc2MDA2OTkzMX0.H9ar1tbQPZS4AMiwNI2M6QpAutb_jQPm-aUPQmNxG0Q'} method=POST url=http://localhost:8000/api/v1/workflows/execute/chat/streaming
2025-10-03T04:18:56.332957Z [debug    ] Reusing existing cache instance [chatter.core.cache_factory] cache_type=general correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.333412Z [debug    ] User found in cache            [chatter.core.auth] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V user_id=01K6B5H7SX4Z260JCRC08RY39A
2025-10-03T04:18:56.336518Z [debug    ] Reusing existing cache instance [chatter.core.cache_factory] cache_type=general correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.337220Z [debug    ] User cached                    [chatter.core.auth] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V user_id=01K6B5H7SX4Z260JCRC08RY39A
2025-10-03T04:18:56.349427Z [debug    ] Retrieved existing conversation 01K6JWZP1QGCAPDBF68FB5T9YN [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.374316Z [info     ] Redis cache connection established [chatter.core.cache] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.375452Z [info     ] Cleared all workflow caches    [chatter.core.workflow_performance] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.375556Z [debug    ] Invalidated workflow caches    [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V workflow_id=01K6M4PR7DWHF4YPR40BTPKTMR
2025-10-03T04:18:56.375612Z [info     ] Created workflow definition 01K6M4PR7DWHF4YPR40BTPKTMR for user 01K6B5H7SX4Z260JCRC08RY39A [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.375667Z [info     ] Created workflow definition 01K6M4PR7DWHF4YPR40BTPKTMR from template 01K6B5H7VZJA79908N4C6G0P7K [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.383690Z [info     ] Created workflow execution 01K6M4PR7R3QH7VJ7QX5N85XF1 for definition 01K6M4PR7DWHF4YPR40BTPKTMR [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.391640Z [info     ] Updated workflow execution 01K6M4PR7R3QH7VJ7QX5N85XF1 [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.391793Z [debug    ] Lazy loaded service: model_registry [chatter.core.dependencies] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.391886Z [debug    ] Reusing existing cache instance [chatter.core.cache_factory] cache_type=general correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.564438Z [debug    ] Lazy loaded service: builtin_tools [chatter.core.dependencies] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
Registered tool: calculator (category: builtin)
Registered tool: get_time (category: builtin)
Registered 2 built-in tools
2025-10-03T04:18:56.568921Z [info     ] Filtered 2 tools down to 2 allowed tools: ['get_time', 'calculator'] [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.569001Z [info     ] Loaded 2 tools for universal template streaming [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.569135Z [info     ] Using memory checkpointer      [chatter.core.langgraph] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.576153Z [info     ] Updated workflow execution 01K6M4PR7R3QH7VJ7QX5N85XF1 [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.576235Z [warning  ] Universal template streaming failed, falling back to dynamic creation: Edge references non-existent target node: END [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.582554Z [info     ] Cleared all workflow caches    [chatter.core.workflow_performance] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.582645Z [debug    ] Invalidated workflow caches    [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V workflow_id=01K6M4PRE05CSBC22ZDAE2MHAZ
2025-10-03T04:18:56.582698Z [info     ] Created workflow definition 01K6M4PRE05CSBC22ZDAE2MHAZ for user 01K6B5H7SX4Z260JCRC08RY39A [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.587467Z [info     ] Created workflow execution 01K6M4PRE6RQV017XF52HNSM5E for definition 01K6M4PRE05CSBC22ZDAE2MHAZ [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.593391Z [info     ] Updated workflow execution 01K6M4PRE6RQV017XF52HNSM5E [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.593527Z [debug    ] Reusing existing cache instance [chatter.core.cache_factory] cache_type=general correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.600189Z [info     ] Filtered 2 tools down to 2 allowed tools: ['get_time', 'calculator'] [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.600274Z [info     ] Loaded 2 tools for streaming workflow execution [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:18:56.600493Z [info     ] Creating workflow nodes        [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retriever=False has_tools=True node_count=3
The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0x789e6d3b8140>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.
/home/yam/.local/lib/python3.12/site-packages/pydantic/_internal/_config.py:94: SAWarning: The garbage collector is trying to clean up non-checked-in connection <AdaptedConnection <asyncpg.connection.Connection object at 0x789e6d3b8140>>, which will be terminated.  Please ensure that SQLAlchemy pooled connections are returned to the pool explicitly, either by calling ``close()`` or by using appropriate context managers to manage their lifecycle.
  def __init__(self, config: ConfigDict | dict[str, Any] | type[Any] | None, *, check: bool = True):
2025-10-03T04:18:56.607413Z [debug    ] Created modern workflow        [chatter.core.langgraph] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V edges=5 nodes=3 recursion_limit=40
2025-10-03T04:18:56.612591Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4aefd605-d031-4257-97be-f228c523ada7', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'what time is it', 'role': 'user'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c860e60>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:18:57.969158Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ba70a102-28b6-40d7-b8c8-1e39e560c8a7', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:18:57.967927', 'role': 'tool', 'tool_call_id': '3VZFgZWmXXsxi3eGT6JWurk6lRc1mEu5'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c861b50>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:18:59.608594Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d49fbf6b-0dd9-49a6-851a-546fb9ba096a', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:18:59.607137', 'role': 'tool', 'tool_call_id': 'yIgYsMlb875PyFifEl31L2qwNZj3mkjq'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c862660>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:01.142623Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1c012e0e-65eb-4f8e-b8bd-ad229aa562b0', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:01.140938', 'role': 'tool', 'tool_call_id': '5kXa0Gb1ampC4Fzdd9zcg7cvyjXSEeFK'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c862270>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:02.591287Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-c48706a4-b24f-4a54-bb05-6a24c6ce4652', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:02.589773', 'role': 'tool', 'tool_call_id': 'iIXdlFK4Y2qWzYvrpBx1xYpVfjWwpTKi'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c6a04a0>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:04.079110Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e51c5adb-b4dd-43ed-a77e-8f83cc313fc5', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:04.077040', 'role': 'tool', 'tool_call_id': 'dUI1kORWd49gCdHVhHmQmPTHSiMBMUXl'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c862e70>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:05.596891Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-73efc7b8-9787-40d3-b45f-ef4b665f2db7', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:05.595187', 'role': 'tool', 'tool_call_id': '2xYUNVHTkhcV7BB42U9qLfAfRLAXMAj8'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c6a1e20>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:07.000368Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d39e1494-567f-4285-ba27-912d229b18a9', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:06.998686', 'role': 'tool', 'tool_call_id': 'B3P6N84I1NCQ68Tcr03NLubJH8gFreBu'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6d1da8d0>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:09.864565Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a11f938d-0aad-45a4-b0f8-6a46dae16f5a', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:09.863059', 'role': 'tool', 'tool_call_id': 'IRpub46uk2BcfP1H5uCKPOcK7FWUSCqj'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c6a2b70>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:12.155045Z [info     ] LLM Node call_model applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-54f4e55b-1f44-4e90-8587-495f3e048dcc', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': '2025-10-02T23:19:12.153289', 'role': 'tool', 'tool_call_id': 'aQ6FKh6HbSAMRQdypdWfxk9Cmksv8YoK'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c6c4320>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:13.650493Z [info     ] LLM Node finalize_response applying context [chatter.core.workflow_graph_builder] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V has_retrieval_context=False retrieval_context_length=0
Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-50478c03-588e-4285-9a9e-1fc43d305124', 'json_data': {'messages': [{'content': 'Provide a final response based on the tool results.', 'role': 'system'}, {'content': '2025-10-02T23:19:13.649230', 'role': 'tool', 'tool_call_id': 'Ph8iXIOFtjqAUTaN2pwN9b2SUxsHkjnw'}], 'model': 'gpt-4', 'max_completion_tokens': 4096, 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'calculator', 'description': 'Perform basic mathematical calculations', 'parameters': {'properties': {'expression': {'type': 'string'}}, 'required': ['expression'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'get_time', 'description': "Get the current date and time. Call this function once if needed, optionally specifying a timezone using 'tz' (e.g., 'America/Denver'). After calling, use the returned time to answer the user's question in natural language.", 'parameters': {'properties': {}, 'type': 'object'}}}]}}
Sending HTTP Request: POST http://localhost:8080/v1/chat/completions
connect_tcp.started host='localhost' port=8080 local_address=None timeout=None socket_options=None
connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x789e6c8061b0>
send_request_headers.started request=<Request [b'POST']>
send_request_headers.complete
send_request_body.started request=<Request [b'POST']>
send_request_body.complete
receive_response_headers.started request=<Request [b'POST']>
receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
HTTP Request: POST http://localhost:8080/v1/chat/completions "HTTP/1.1 200 OK"
HTTP Response: POST http://localhost:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
request_id: None
receive_response_body.started request=<Request [b'POST']>
receive_response_body.complete
response_closed.started
response_closed.complete
close.started
close.complete
2025-10-03T04:19:15.441895Z [debug    ] HTTP Response                  [chatter.main] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V duration=19.11158061027527 duration_ms=19111.58061027527 headers={'cache-control': 'no-cache', 'connection': 'keep-alive', 'content-type': 'text/event-stream; charset=utf-8', 'x-content-type-options': 'nosniff', 'x-frame-options': 'DENY', 'x-xss-protection': '1; mode=block', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'referrer-policy': 'strict-origin-when-cross-origin', 'x-correlation-id': '01K6M4PR6B1YRKD9RS0HJ3CZ1V', 'x-ratelimit-limit': '100', 'x-ratelimit-remaining': '99', 'x-ratelimit-reset': '1759465196', 'x-ratelimit-window': '60'} status_code=200
2025-10-03T04:19:15.442139Z [debug    ] Recorded request metrics       [chatter.core.monitoring] method=POST path=/api/v1/workflows/execute/chat/streaming response_time_ms=19111.58061027527
INFO:     127.0.0.1:55619 - "POST /api/v1/workflows/execute/chat/streaming HTTP/1.1" 200 OK
2025-10-03T04:19:15.451824Z [info     ] Updated workflow execution 01K6M4PRE6RQV017XF52HNSM5E [chatter.services.workflow_management] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
2025-10-03T04:19:15.451972Z [info     ] Dynamic workflow streaming execution 01K6M4PRE6RQV017XF52HNSM5E completed successfully [chatter.services.workflow_execution] correlation_id=01K6M4PR6B1YRKD9RS0HJ3CZ1V
